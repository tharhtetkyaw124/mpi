MPI Examples to run 
A Parallel Data Structure
 This assignment implements a simple parallel data structure. This structure is a two dimension 
regular mesh of points, divided into slabs, with each slab allocated to a different processor. In the 
simplest C form, the full data structure is
 double x[maxn][maxn]; 
and we want to arrange it so that each processor has a local piece:
 double xlocal[maxn][maxn/size]; 
where 
size
 is the size of the communicator (e.g., the number of processors).
 If that was all that there was to it, there wouldn't be anything to do. However, for the computation 
that we're going to perform on this data structure, we'll need the adjacent values. That is, to 
compute a new 
x[i][j]
 x[i][j+1] 
x[i][j-1] 
x[i+1][j] 
x[i-1][j] 
, we will need 
The last two of these could be a problem if they are not in 
processors. To handle this difficulty, we define 
xlocal
 ghost points
 but are instead on the adjacent 
that we will contain the values of these 
adjacent points.
 Write code to copy divide the array x into equal-sized strips and to copy the adjacent edges to the 
neighboring processors. Assume that x is maxn by maxn, and that maxn is evenly divided by the 
number of processors. For simplicity, You may assume a fixed size array and a fixed (or 
minimum) number of processors. 
To test the routine, have each processor fill its section with the rank of the process, and the 
ghostpoints with -1. After the exchange takes place, test to make sure that the ghostpoints have 
the proper value. Assume that the domain is not periodic; that is, the top process (rank = size - 1) 
only sends and receives data from the one under it (rank = size - 2) and the bottom process (rank 
= 0) only sends and receives data from the one above it (rank = 1). Consider a maxn of 12 and use 
4 processors to start with. 
For this exercise, use MPI_Send and MPI_Recv. See the related exercises for alternatives that use 
the nonblocking operations or MPI_SendRecv.
 A more detailed description of this operation may be found in Chapter 4 of "Using MPI". 
Page 1 of 15 
Page 2 of 15 
 
You may want to use these MPI routines in your solution: 
MPI_Send MPI_Recv 
#include <stdio.h> 
#include "mpi.h" 
 
/* This example handles a 12 x 12 mesh, on 4 processors only. */ 
#define maxn 12 
 
int main( argc, argv ) 
int argc; 
char **argv; 
{ 
    int rank, value, size, errcnt, toterr, i, j; 
    MPI_Status status; 
    double x[12][12]; 
    double xlocal[(12/4)+2][12]; 
 
    MPI_Init( &argc, &argv ); 
 
    MPI_Comm_rank( MPI_COMM_WORLD, &rank ); 
    MPI_Comm_size( MPI_COMM_WORLD, &size ); 
 
    if (size != 4) MPI_Abort( MPI_COMM_WORLD, 1 ); 
 
    /* xlocal[][0] is lower ghostpoints, xlocal[][maxn+2] is upper */ 
 
    /* Fill the data as specified */ 
    for (i=1; i<=maxn/size; i++)  
 for (j=0; j<maxn; j++)  
     xlocal[i][j] = rank; 
    for (j=0; j<maxn; j++) { 
 xlocal[0][j] = -1; 
 xlocal[maxn/size+1][j] = -1; 
    } 
 
    /* Send up unless I'm at the top, then receive from below */ 
    /* Note the use of xlocal[i] for &xlocal[i][0] */ 
    if (rank < size - 1)  
 MPI_Send( xlocal[maxn/size], maxn, MPI_DOUBLE, rank + 1, 0,  
    MPI_COMM_WORLD ); 
    if (rank > 0) 
 MPI_Recv( xlocal[0], maxn, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD,  
    &status ); 
    /* Send down unless I'm at the bottom */ 
    if (rank > 0)  
 MPI_Send( xlocal[1], maxn, MPI_DOUBLE, rank - 1, 1, MPI_COMM_WORLD ); 
    if (rank < size - 1)  
 MPI_Recv( xlocal[maxn/size+1], maxn, MPI_DOUBLE, rank + 1, 1,  
    MPI_COMM_WORLD, &status ); 
 
    /* Check that we have the correct results */ 
    errcnt = 0; 
    for (i=1; i<=maxn/size; i++)  
 for (j=0; j<maxn; j++)  
     if (xlocal[i][j] != rank) errcnt++; 
    for (j=0; j<maxn; j++) { 
 if (xlocal[0][j] != rank - 1) errcnt++; 
 if (rank < size-1 && xlocal[maxn/size+1][j] != rank + 1) errcnt++; 
    } 
 
    MPI_Reduce( &errcnt, &toterr, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD ); 
    if (rank == 0) { 
 if (toterr) 
     printf( "! found %d errors\n", toterr ); 
 else 
} 
printf( "No errors\n" ); 
MPI_Finalize( ); 
return 0; 
} 
//////////////////////////////////// 
A simple Jacobi iteration 
In this example, you will put together some of the previous examples to implement a simple Jacobi 
iteration for approximating the solution to a linear system of equations. 
In this example, we solve the Laplace equation in two dimensions with finite differences. This 
may sound involved, but really amount only to a simple computation, combined with the previous 
example of a parallel mesh data structure. 
Any numerical analysis text will show that iterating 
while (not converged) { 
for (i,j) 
xnew[i][j] = (x[i+1][j] + x[i-1][j] + x[i][j+1] + x[i][j-1])/4; 
for (i,j) 
x[i][j] = xnew[i][j]; 
} 
will compute an approximation for the solution of Laplace's equation. There is one last detail; this 
replacement of xnew with the average of the values around it is applied only in the interior; the 
boundary values are left fixed. In practice, this means that if the mesh is n by n, then the values 
x[0][j] 
x[n-1][j] 
x[i][0] 
x[i][n-1] 
are left unchanged. Of course, these refer to the complete mesh; you'll have to figure out what to 
do with for the decomposed data structures (xlocal). 
Because the values are replaced by averaging around them, these techniques are called relaxation 
methods. 
We wish to compute this approximation in parallel. Write a program to apply this approximation. 
For convergence testing, compute 
diffnorm = 0; 
for (i,j) 
diffnorm += (xnew[i][j] - x[i][j]) * (xnew[i][j] - x[i][j]); 
diffnorm = sqrt(diffnorm); 
You'll need to use MPI_Allreduce for this. (Why not use MPI_Reduce?) Have process zero write 
out the value of diffnorm and the iteration count at each iteration. When diffnorm is less that 1.0e
2, consider the iteration converged. Also, if you reach 100 iterations, exit the loop. 
For simplicity, consider a 12 x 12 mesh on 4 processors.  
Page 3 of 15 
The example solution uses the boundary values from the previous exercise; they are -1 on the top 
and bottom, and the rank of the process on the side. The initial data (the values of x that are being 
relaxed) are also the same; the interior points have the same value as the rank of the process. This 
is shown below: -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  
3  3  3  3  3  3  3  3  3  3  3  3 
3  3  3  3  3  3  3  3  3  3  3  3 
2  2  2  2  2  2  2  2  2  2  2  2 
2  2  2  2  2  2  2  2  2  2  2  2 
2  2  2  2  2  2  2  2  2  2  2  2 
1  1  1  1  1  1  1  1  1  1  1  1 
1  1  1  1  1  1  1  1  1  1  1  1 
1  1  1  1  1  1  1  1  1  1  1  1 
0  0  0  0  0  0  0  0  0  0  0  0 
0  0  0  0  0  0  0  0  0  0  0  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  
Note that this is a very poor way to solve this numerical problem, and this method is being used 
only because it is very simple. Fortunately, the MPI parts of this example are very similar to those 
that are used in the better parallel algorithms for this problem. In particular, the use of ghost points 
in the parallel data structure is very similar to what is used in methods such as Conjugate Gradient 
or Multigrid. 
You may want to use these MPI routines in your solution: 
MPI_Init MPI_Finalize MPI_Send MPI_Recv MPI_Allreduce MPI_Comm_size MPI_Comm_r
 ank 
#include <stdio.h> 
#include <math.h> 
#include "mpi.h" 
/* This example handles a 12 x 12 mesh, on 4 processors only. */ 
#define maxn 12 
int main( argc, argv ) 
int argc; 
char **argv; 
Page 4 of 15 
Page 5 of 15 
 
{ 
    int        rank, value, size, errcnt, toterr, i, j, itcnt; 
    int        i_first, i_last; 
    MPI_Status status; 
    double     diffnorm, gdiffnorm; 
    double     xlocal[(12/4)+2][12]; 
    double     xnew[(12/3)+2][12]; 
 
    MPI_Init( &argc, &argv ); 
 
    MPI_Comm_rank( MPI_COMM_WORLD, &rank ); 
    MPI_Comm_size( MPI_COMM_WORLD, &size ); 
 
    if (size != 4) MPI_Abort( MPI_COMM_WORLD, 1 ); 
 
    /* xlocal[][0] is lower ghostpoints, xlocal[][maxn+2] is upper */ 
 
    /* Note that top and bottom processes have one less row of interior 
       points */ 
    i_first = 1; 
    i_last  = maxn/size; 
    if (rank == 0)        i_first++; 
    if (rank == size - 1) i_last--; 
 
    /* Fill the data as specified */ 
    for (i=1; i<=maxn/size; i++)  
 for (j=0; j<maxn; j++)  
     xlocal[i][j] = rank; 
    for (j=0; j<maxn; j++) { 
 xlocal[i_first-1][j] = -1; 
 xlocal[i_last+1][j] = -1; 
    } 
 
    itcnt = 0; 
    do { 
 /* Send up unless I'm at the top, then receive from below */ 
 /* Note the use of xlocal[i] for &xlocal[i][0] */ 
 if (rank < size - 1)  
     MPI_Send( xlocal[maxn/size], maxn, MPI_DOUBLE, rank + 1, 0,  
        MPI_COMM_WORLD ); 
 if (rank > 0) 
     MPI_Recv( xlocal[0], maxn, MPI_DOUBLE, rank - 1, 0,  
        MPI_COMM_WORLD, &status ); 
 /* Send down unless I'm at the bottom */ 
 if (rank > 0)  
     MPI_Send( xlocal[1], maxn, MPI_DOUBLE, rank - 1, 1,  
        MPI_COMM_WORLD ); 
 if (rank < size - 1)  
     MPI_Recv( xlocal[maxn/size+1], maxn, MPI_DOUBLE, rank + 1, 1,  
        MPI_COMM_WORLD, &status ); 
  
 /* Compute new values (but not on boundary) */ 
 itcnt ++; 
 diffnorm = 0.0; 
 for (i=i_first; i<=i_last; i++)  
     for (j=1; j<maxn-1; j++) { 
  xnew[i][j] = (xlocal[i][j+1] + xlocal[i][j-1] + 
         xlocal[i+1][j] + xlocal[i-1][j]) / 4.0; 
  diffnorm += (xnew[i][j] - xlocal[i][j]) *  
              (xnew[i][j] - xlocal[i][j]); 
     } 
 /* Only transfer the interior points */ 
 for (i=i_first; i<=i_last; i++)  
     for (j=1; j<maxn-1; j++)  
  xlocal[i][j] = xnew[i][j]; 
 
 MPI_Allreduce( &diffnorm, &gdiffnorm, 1, MPI_DOUBLE, MPI_SUM, 
Page 6 of 15 
 
         MPI_COMM_WORLD ); 
 gdiffnorm = sqrt( gdiffnorm ); 
 if (rank == 0) printf( "At iteration %d, diff is %e\n", itcnt,  
          gdiffnorm ); 
    } while (gdiffnorm > 1.0e-2 && itcnt < 100); 
 
    MPI_Finalize( ); 
    return 0; 
} 
////////////////////////////////////////// 
Collecting Data 
Once a parallel program computes a solution, it it often necessary to write the solution out to disk 
or display it on the user's terminal. This often means collecting the data onto a single processor 
that handles performing the output (since there are few parallel file systems around). This exercise 
shows one way in which this may be performed. Take your Jacobi iteration example and modify 
it so that the computed solution is collected onto process 0, which then writes the solution out (to 
standard output). You might want to write the results out in a form that can be used for display 
with tools like gnuplot or matlab, but this is not required. You may assume that process zero can 
store the entire solution. Also, assume that each process contributes exactly the same amount of 
data (see the related problems for the general case). 
You may want to use these MPI routines in your solution: MPI_Gather 
#include <stdio.h> 
#include <math.h> 
#include "mpi.h" 
 
/* This example handles a 12 x 12 mesh, on 4 processors only. */ 
#define maxn 12 
 
int main( argc, argv ) 
int argc; 
char **argv; 
{ 
    Int rank, value, size, errcnt, toterr, i, j, itcnt; 
    int i_first, i_last; 
    MPI_Status status; 
    double diffnorm, gdiffnorm; 
    double xlocal[(12/4)+2][12]; 
    double xnew[(12/3)+2][12]; 
    double x[maxn][maxn]; 
 
    MPI_Init( &argc, &argv ); 
 
    MPI_Comm_rank( MPI_COMM_WORLD, &rank ); 
    MPI_Comm_size( MPI_COMM_WORLD, &size ); 
 
    if (size != 4) MPI_Abort( MPI_COMM_WORLD, 1 ); 
 
    /* xlocal[][0] is lower ghostpoints, xlocal[][maxn+2] is upper */ 
 
    /* Note that top and bottom processes have one less row of interior 
       points */ 
    i_first = 1; 
    i_last  = maxn/size; 
    if (rank == 0)        i_first++; 
    if (rank == size - 1) i_last--; 
 
    /* Fill the data as specified */ 
    for (i=1; i<=maxn/size; i++)  
Page 7 of 15 
 
 for (j=0; j<maxn; j++)  
     xlocal[i][j] = rank; 
    for (j=0; j<maxn; j++) { 
 xlocal[i_first-1][j] = -1; 
 xlocal[i_last+1][j] = -1; 
    } 
 
    itcnt = 0; 
    do { 
 /* Send up unless I'm at the top, then receive from below */ 
 /* Note the use of xlocal[i] for &xlocal[i][0] */ 
 if (rank < size - 1)  
     MPI_Send( xlocal[maxn/size], maxn, MPI_DOUBLE, rank + 1, 0,  
        MPI_COMM_WORLD ); 
 if (rank > 0) 
     MPI_Recv( xlocal[0], maxn, MPI_DOUBLE, rank - 1, 0,  
        MPI_COMM_WORLD, &status ); 
 /* Send down unless I'm at the bottom */ 
 if (rank > 0)  
     MPI_Send( xlocal[1], maxn, MPI_DOUBLE, rank - 1, 1,  
        MPI_COMM_WORLD ); 
 if (rank < size - 1)  
     MPI_Recv( xlocal[maxn/size+1], maxn, MPI_DOUBLE, rank + 1, 1,  
        MPI_COMM_WORLD, &status ); 
  
 /* Compute new values (but not on boundary) */ 
 itcnt ++; 
 diffnorm = 0.0; 
 for (i=i_first; i<=i_last; i++)  
     for (j=1; j<maxn-1; j++) { 
  xnew[i][j] = (xlocal[i][j+1] + xlocal[i][j-1] + 
         xlocal[i+1][j] + xlocal[i-1][j]) / 4.0; 
  diffnorm += (xnew[i][j] - xlocal[i][j]) *  
              (xnew[i][j] - xlocal[i][j]); 
     } 
 /* Only transfer the interior points */ 
 for (i=i_first; i<=i_last; i++)  
     for (j=1; j<maxn-1; j++)  
  xlocal[i][j] = xnew[i][j]; 
 
 MPI_Allreduce( &diffnorm, &gdiffnorm, 1, MPI_DOUBLE, MPI_SUM, 
         MPI_COMM_WORLD ); 
 gdiffnorm = sqrt( gdiffnorm ); 
 if (rank == 0) printf( "At iteration %d, diff is %e\n", itcnt,  
          gdiffnorm ); 
    } while (gdiffnorm > 1.0e-2 && itcnt < 100); 
 
    /* Collect the data into x and print it */ 
    /* This code (and the declaration of x above) are the only changes to the 
       Jacobi code */ 
    MPI_Gather( xlocal[1], maxn * (maxn/size), MPI_DOUBLE, 
  x, maxn * (maxn/size), MPI_DOUBLE,  
  0, MPI_COMM_WORLD ); 
    if (rank == 0) { 
 printf( "Final solution is\n" ); 
 for (i=maxn-1; i>=0; i--) { 
     for (j=0; j<maxn; j++)  
  printf( "%f ", x[i][j] ); 
     printf( "\n" ); 
 } 
    } 
 
    MPI_Finalize( ); 
    return 0; 
} 
////////////////////////////// 
Page 8 of 15 
 
Putting it all together: A complete application 
In this exercise, you will put together parts from the previous exercise to provide a simple linear 
equation solver. 
To make this a complete parallel program, we must handle the issue of input and output. Rather 
than specify the values of the boundary conditions of the distributed array, these values should be 
read in from a file. Assume that the file is in the same format as the output data from the Collecting 
data exercise. 
Have process zero read this data and send it to the other processes. A Sample data file is available. 
Use the Jacobi linear solver from the previous examples, as well as the data output from the 
"Collecting data" example. 
Once your solution is working, consider enhancing it by 
 Using topologies to find the neighbors 
 Using some of the alternative strategies from A Parallel Data Structure 
 Allowing each processor to have a different number of rows of the mesh 
You may want to use these MPI routines in your solution: 
MPI_Scatterv 
#include <stdio.h> 
#include <math.h> 
#include "mpi.h" 
 
/* This example handles a 12 x 12 mesh, on 4 processors only. */ 
#define maxn 12 
 
int main( argc, argv ) 
int argc; 
char **argv; 
{ 
    int        rank, value, size, errcnt, toterr, i, j, itcnt; 
    int        i_first, i_last; 
    MPI_Status status; 
    double     diffnorm, gdiffnorm; 
    double     xlocal[(12/4)+2][12]; 
    double     xnew[(12/3)+2][12]; 
    double     x[maxn][maxn]; 
 
    MPI_Init( &argc, &argv ); 
 
    MPI_Comm_rank( MPI_COMM_WORLD, &rank ); 
    MPI_Comm_size( MPI_COMM_WORLD, &size ); 
 
    if (size != 4) MPI_Abort( MPI_COMM_WORLD, 1 ); 
 
    /* xlocal[][0] is lower ghostpoints, xlocal[][maxn+2] is upper */ 
 
    /* Read the data from the named file */ 
    if (rank == 0) { 
 FILE *fp; 
 fp =fopen( "in.dat", "r" ); 
 if (!fp) MPI_Abort( MPI_COMM_WORLD, 1 ); 
 /* This includes the top and bottom edge */ 
 for (i=maxn-1; i>=0; i--) { 
     for (j=0; j<maxn; j++) { 
Page 9 of 15 
 
  fscanf( fp, "%lf", &x[i][j] ); 
     } 
     fscanf( fp, "\n" ); 
 } 
    } 
 
    MPI_Scatter( x[0], maxn * (maxn/size), MPI_DOUBLE,  
   xlocal[1], maxn * (maxn/size), MPI_DOUBLE,  
   0, MPI_COMM_WORLD ); 
 
    /* Note that top and bottom processes have one less row of interior 
       points */ 
    i_first = 1; 
    i_last  = maxn/size; 
    if (rank == 0)        i_first++; 
    if (rank == size - 1) i_last--; 
 
    itcnt = 0; 
    do { 
 /* Send up unless I'm at the top, then receive from below */ 
 /* Note the use of xlocal[i] for &xlocal[i][0] */ 
 if (rank < size - 1)  
     MPI_Send( xlocal[maxn/size], maxn, MPI_DOUBLE, rank + 1, 0,  
        MPI_COMM_WORLD ); 
 if (rank > 0) 
     MPI_Recv( xlocal[0], maxn, MPI_DOUBLE, rank - 1, 0,  
        MPI_COMM_WORLD, &status ); 
 /* Send down unless I'm at the bottom */ 
 if (rank > 0)  
     MPI_Send( xlocal[1], maxn, MPI_DOUBLE, rank - 1, 1,  
        MPI_COMM_WORLD ); 
 if (rank < size - 1)  
     MPI_Recv( xlocal[maxn/size+1], maxn, MPI_DOUBLE, rank + 1, 1,  
        MPI_COMM_WORLD, &status ); 
  
 /* Compute new values (but not on boundary) */ 
 itcnt ++; 
 diffnorm = 0.0; 
 for (i=i_first; i<=i_last; i++)  
     for (j=1; j<maxn-1; j++) { 
  xnew[i][j] = (xlocal[i][j+1] + xlocal[i][j-1] + 
         xlocal[i+1][j] + xlocal[i-1][j]) / 4.0; 
  diffnorm += (xnew[i][j] - xlocal[i][j]) *  
              (xnew[i][j] - xlocal[i][j]); 
     } 
 /* Only transfer the interior points */ 
 for (i=i_first; i<=i_last; i++)  
     for (j=1; j<maxn-1; j++)  
  xlocal[i][j] = xnew[i][j]; 
 
 MPI_Allreduce( &diffnorm, &gdiffnorm, 1, MPI_DOUBLE, MPI_SUM, 
         MPI_COMM_WORLD ); 
 gdiffnorm = sqrt( gdiffnorm ); 
 if (rank == 0) printf( "At iteration %d, diff is %e\n", itcnt,  
          gdiffnorm ); 
    } while (gdiffnorm > 1.0e-2 && itcnt < 100); 
 
    /* Collect the data into x and print it */ 
    /* This code (and the declaration of x above) are the only changes to the 
       Jacobi code */ 
    MPI_Gather( xlocal[1], maxn * (maxn/size), MPI_DOUBLE, 
  x, maxn * (maxn/size), MPI_DOUBLE,  
  0, MPI_COMM_WORLD ); 
    if (rank == 0) { 
 printf( "Final solution is\n" ); 
 for (i=maxn-1; i>=0; i--) { 
     for (j=0; j<maxn; j++)  
Page 10 of 15 
 
  printf( "%f ", x[i][j] ); 
     printf( "\n" ); 
 } 
    } 
 
    MPI_Finalize( ); 
    return 0; 
} 
 
///////////////////////////////// 
Exchanging data with MPI_Sendrecv 
In this exercise, use MPI_Sendrecv to exchange data with the neighboring processors. That is, 
processors 0 and 1 exchange, 2 and 3 exchange, etc. Then 1 and 2 exchange, 3 and 4, etc. This 
"head-to-head" exchange may be more efficient on some systems. 
You may want to use these MPI routines in your solution: 
MPI_Sendrecv 
#include <stdio.h> 
#include "mpi.h" 
 
/* This example handles a 12 x 12 mesh, on 4 processors only. */ 
#define maxn 12 
 
int main( argc, argv ) 
int argc; 
char **argv; 
{ 
    int rank, value, size, errcnt, toterr, i, j; 
    int up_nbr, down_nbr; 
    MPI_Status status; 
    double x[12][12]; 
    double xlocal[(12/4)+2][12]; 
 
    MPI_Init( &argc, &argv ); 
 
    MPI_Comm_rank( MPI_COMM_WORLD, &rank ); 
    MPI_Comm_size( MPI_COMM_WORLD, &size ); 
 
    if (size != 4) MPI_Abort( MPI_COMM_WORLD, 1 ); 
 
    /* xlocal[][0] is lower ghostpoints, xlocal[][maxn+2] is upper */ 
 
    /* Fill the data as specified */ 
    for (i=1; i<=maxn/size; i++)  
 for (j=0; j<maxn; j++)  
     xlocal[i][j] = rank; 
    for (j=0; j<maxn; j++) { 
 xlocal[0][j] = -1; 
 xlocal[maxn/size+1][j] = -1; 
    } 
 
    /* Processors 0 and 1 exchange, 2 and 3 exchange, etc.  Then 
       1 and 2 exchange, 3 and 4, etc.  The formula for this is 
       if (even) exchng up else down 
       if (odd)  exchng up else down 
       */ 
    /* Note the use of xlocal[i] for &xlocal[i][0] */ 
    /* Note that we use MPI_PROC_NULL to remove the if statements that 
       would be needed without MPI_PROC_NULL */ 
    up_nbr = rank + 1; 
    if (up_nbr >= size) up_nbr = MPI_PROC_NULL; 
Page 11 of 15 
 
    down_nbr = rank - 1; 
    if (down_nbr < 0) down_nbr = MPI_PROC_NULL; 
 
    if ((rank % 2) == 0) { 
 /* exchange up */ 
 MPI_Sendrecv( xlocal[maxn/size], maxn, MPI_DOUBLE, up_nbr, 0,  
        xlocal[maxn/size+1], maxn, MPI_DOUBLE, up_nbr, 0,  
        MPI_COMM_WORLD, &status ); 
    } 
    else { 
 /* exchange down */ 
 MPI_Sendrecv( xlocal[1], maxn, MPI_DOUBLE, down_nbr, 0, 
        xlocal[0], maxn, MPI_DOUBLE, down_nbr, 0,  
        MPI_COMM_WORLD, &status ); 
    } 
 
    /* Do the second set of exchanges */ 
    if ((rank % 2) == 1) { 
 /* exchange up */ 
 MPI_Sendrecv( xlocal[maxn/size], maxn, MPI_DOUBLE, up_nbr, 1,  
        xlocal[maxn/size+1], maxn, MPI_DOUBLE, up_nbr, 1,  
        MPI_COMM_WORLD, &status ); 
    } 
    else { 
 /* exchange down */ 
 MPI_Sendrecv( xlocal[1], maxn, MPI_DOUBLE, down_nbr, 1, 
        xlocal[0], maxn, MPI_DOUBLE, down_nbr, 1,  
        MPI_COMM_WORLD, &status ); 
    } 
     
    /* Check that we have the correct results */ 
    errcnt = 0; 
    for (i=1; i<=maxn/size; i++)  
 for (j=0; j<maxn; j++)  
     if (xlocal[i][j] != rank) errcnt++; 
    for (j=0; j<maxn; j++) { 
 if (xlocal[0][j] != rank - 1) errcnt++; 
 if (rank < size-1 && xlocal[maxn/size+1][j] != rank + 1) errcnt++; 
    } 
 
    MPI_Reduce( &errcnt, &toterr, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD ); 
    if (rank == 0) { 
 if (toterr) 
     printf( "! found %d errors\n", toterr ); 
 else 
     printf( "No errors\n" ); 
    } 
 
    MPI_Finalize( ); 
    return 0; 
} 
/////////////////////////////////// 
Collecting Data (with varying amounts from each process) 
In many cases, it is impossible to divide a parallel data structure so that each process has exactly 
the same amount of data. It may not even be desirable, if the amount of work to be done varies. 
Modify your code so that each process can have a different number of rows of the distributed 
mesh. 
You may want to use these MPI routines in your solution: 
MPI_Gather MPI_Gatherv 
 
Page 12 of 15 
 
#include <stdio.h> 
#include <stdlib.h> 
#include <math.h> 
#include "mpi.h" 
 
/* This example handles a 12 x 12 mesh, on 4 processors only. */ 
#define maxn 12 
/* This example handles precisely 4 processors */ 
#define maxp 4 
 
int main( argc, argv ) 
int argc; 
char **argv; 
{ 
    int        rank, value, size, errcnt, toterr, i, j, itcnt; 
    int        i_first, i_last; 
    MPI_Status status; 
    double     diffnorm, gdiffnorm; 
    double     xlocal[(12/4)+2][12]; 
    double     xnew[(12/3)+2][12]; 
    /* For the gatherv */ 
    double     x[maxn][maxn]; 
    int        lcnt; 
    int        recvcnts[maxp]; 
    int        displs[maxp]; 
 
    MPI_Init( &argc, &argv ); 
 
    MPI_Comm_rank( MPI_COMM_WORLD, &rank ); 
    MPI_Comm_size( MPI_COMM_WORLD, &size ); 
 
    if (size != 4) MPI_Abort( MPI_COMM_WORLD, 1 ); 
 
    /* xlocal[][0] is lower ghostpoints, xlocal[][maxn+2] is upper */ 
    /* Note that top and bottom processes have one less row of interior 
       points */ 
    i_first = 1; 
    i_last  = maxn/size; 
    if (rank == 0)        i_first++; 
    if (rank == size - 1) i_last--; 
 
    /* Fill the data as specified */ 
    for (i=1; i<=maxn/size; i++)  
 for (j=0; j<maxn; j++)  
     xlocal[i][j] = rank; 
    for (j=0; j<maxn; j++) { 
 xlocal[i_first-1][j] = -1; 
 xlocal[i_last+1][j] = -1; 
    } 
 
    itcnt = 0; 
    do { 
 /* Send up unless I'm at the top, then receive from below */ 
 /* Note the use of xlocal[i] for &xlocal[i][0] */ 
 if (rank < size - 1)  
     MPI_Send( xlocal[maxn/size], maxn, MPI_DOUBLE, rank + 1, 0,  
        MPI_COMM_WORLD ); 
 if (rank > 0) 
     MPI_Recv( xlocal[0], maxn, MPI_DOUBLE, rank - 1, 0,  
        MPI_COMM_WORLD, &status ); 
 /* Send down unless I'm at the bottom */ 
 if (rank > 0)  
     MPI_Send( xlocal[1], maxn, MPI_DOUBLE, rank - 1, 1,  
        MPI_COMM_WORLD ); 
 if (rank < size - 1)  
     MPI_Recv( xlocal[maxn/size+1], maxn, MPI_DOUBLE, rank + 1, 1,  
        MPI_COMM_WORLD, &status ); 
Page 13 of 15 
 
  
 /* Compute new values (but not on boundary) */ 
 itcnt ++; 
 diffnorm = 0.0; 
 for (i=i_first; i<=i_last; i++)  
     for (j=1; j<maxn-1; j++) { 
  xnew[i][j] = (xlocal[i][j+1] + xlocal[i][j-1] + 
         xlocal[i+1][j] + xlocal[i-1][j]) / 4.0; 
  diffnorm += (xnew[i][j] - xlocal[i][j]) *  
              (xnew[i][j] - xlocal[i][j]); 
     } 
 /* Only transfer the interior points */ 
 for (i=i_first; i<=i_last; i++)  
     for (j=1; j<maxn-1; j++)  
  xlocal[i][j] = xnew[i][j]; 
 
 MPI_Allreduce( &diffnorm, &gdiffnorm, 1, MPI_DOUBLE, MPI_SUM, 
         MPI_COMM_WORLD ); 
 gdiffnorm = sqrt( gdiffnorm ); 
 if (rank == 0) printf( "At iteration %d, diff is %e\n", itcnt,  
          gdiffnorm ); 
    } while (gdiffnorm > 1.0e-2 && itcnt < 100); 
 
    /* Collect the data into x and print it */ 
    /* This code (and the declaration of x above) are the only changes to the 
       Jacobi code */ 
    /* Use gather to get the recv counts */ 
    lcnt = maxn * (maxn / size); 
    MPI_Gather( &lcnt, 1, MPI_INT, recvcnts, 1, MPI_INT, 0, MPI_COMM_WORLD ); 
    /* Form the displacements */ 
    displs[0] = 0; 
    for (i=1; i<size; i++)  
 displs[i] = displs[i-1] + recvcnts[i-1]; 
 
    MPI_Gatherv( xlocal[1], lcnt, MPI_DOUBLE, 
  x, recvcnts, displs, MPI_DOUBLE,  
  0, MPI_COMM_WORLD ); 
    if (rank == 0) { 
 printf( "Final solution is\n" ); 
 for (i=maxn-1; i>=0; i--) { 
     for (j=0; j<maxn; j++)  
  printf( "%f ", x[i][j] ); 
     printf( "\n" ); 
 } 
    } 
 
    MPI_Finalize( ); 
    return 0; 
} 
/////////////////////////////////////// 
Using Datatypes to Scatter Matrices Among Processors 
Let a n by m matrix (A(n,m) in Fortran) be created on processor 0. For example, it might be read 
in from memory, or it might have been computed. Assume that there are four processors, and 
processor zero will send a part of this matrix to the other processors. Processor 1 gets A(i,j) for 
i=n/2+1,...,n, and j=1,...,m/2. Processor 2 gets A(i,j) for i=1,...,n/2 and j=m/2+1,...,m, and 
processor 3 gets A(i,j) for i=n/2+1,...,n and j=m/2,...,m . This is just a two-dimensional 
decomposition of A across four processors. Use MPI_Scatterv to send the data from process 0 
to all other processes (including process 0). For simplicity, choose n = m = 8. 
#include <stdio.h> 
#include "mpi.h" 
Page 14 of 15 
 
 
int main( argc, argv ) 
int argc; 
char *argv[]; 
{ 
    double A[8][8], alocal[4][4]; 
    int i, j, r, rank, size; 
    MPI_Datatype stype, t[2], vtype; 
    MPI_Aint displs[2]; 
    int blklen[2]; 
    int sendcount[4], sdispls[4]; 
 
    MPI_Init( &argc, &argv ); 
    MPI_Comm_rank( MPI_COMM_WORLD, &rank ); 
    MPI_Comm_size( MPI_COMM_WORLD, &size ); 
    if (size != 4) { 
 fprintf( stderr, "This program requires exactly four processors\n" ); 
 MPI_Abort( MPI_COMM_WORLD, 1 ); 
    } 
    if (rank == 0) { 
 /* Initialize the matrix.  Note that C has row-major storage */ 
 for (j=0; j<8; j++)  
     for (i=0; i<8; i++) 
  A[i][j] = 1.0 + i / 10.0 + j / 100.0; 
 /* Form the vector type for the submatrix */ 
 MPI_Type_vector( 4, 4, 8, MPI_DOUBLE, &vtype ); 
 /* Set an UB so that we can place this in the matrix */ 
 t[0] = vtype; 
 t[1] = MPI_UB; 
 displs[0] = 0; 
 displs[1] = 4 * sizeof(double); 
 blklen[0] = 1; 
 blklen[1] = 1; 
 MPI_Type_struct( 2, blklen, displs, t, &stype ); 
 MPI_Type_commit( &stype ); 
 /* Setup the Scatter values for the send buffer */ 
 sendcount[0] = 1; 
 sendcount[1] = 1; 
 sendcount[2] = 1; 
 sendcount[3] = 1; 
 sdispls[0] = 0; 
 sdispls[1] = 1; 
 sdispls[2] = 8; 
 sdispls[3] = 9; 
 MPI_Scatterv( &A[0][0], sendcount, sdispls, stype,  
        &alocal[0][0], 4*4, MPI_DOUBLE, 0, MPI_COMM_WORLD ); 
         
    } 
    else { 
     MPI_Scatterv( (void *)0, (void *)0, (void *)0, MPI_DATATYPE_NULL, 
        &alocal[0][0], 4*4, MPI_DOUBLE, 0, MPI_COMM_WORLD ); 
    } 
 
    /* Everyone can now print their local matrix */ 
    for (r = 0; r<size; r++) { 
 if (rank == r) { 
     printf( "Output for process %d\n", r ); 
     for (j=0; j<4; j++) { 
  for (i=0; i<4; i++)  
      printf( "%.2f ", alocal[i][j] ); 
  printf( "\n" ); 
     } 
     fflush( stdout ); 
 } 
 MPI_Barrier( MPI_COMM_WORLD ); 
    } 
 
} 
MPI_Finalize( ); 
return 0; 
/////////////////////////////////// 
Page 15 of 15 